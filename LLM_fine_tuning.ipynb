{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4cd6afa-08cb-4f2b-bfec-89f00eef2415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenCV: 4.12.0\n",
      "NumPy: 2.2.6\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy\n",
    "print(\"OpenCV:\", cv2.__version__)\n",
    "print(\"NumPy:\", numpy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24fb8d7e-9218-46fb-a617-81b12ebac65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\SRI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import MarianTokenizer, MarianMTModel\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75cee3f0-8dfe-4256-800f-7a5716c88736",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b84ba08-13ab-446a-bfaa-b23f2469d4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def load_data(data_path):\n",
    "    df = pd.read_csv(data_path)\n",
    "    eng_col, fr_col = df.columns[:2]\n",
    "    eng_sentences = [preprocess_text(str(x)) for x in df[eng_col]]\n",
    "    fr_sentences = [preprocess_text(str(x)) for x in df[fr_col]]\n",
    "    pairs = [(e, f) for e, f in zip(eng_sentences, fr_sentences) if e and f]\n",
    "    return zip(*pairs)\n",
    "\n",
    "eng_sentences, fr_sentences = load_data(\"eng_-french.csv\")\n",
    "\n",
    "eng_train, eng_test, fr_train, fr_test = train_test_split(list(eng_sentences), list(fr_sentences), test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e555d6b7-6fd4-4e19-a8ad-5295be028328",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SRI\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MarianMTModel(\n",
       "  (model): MarianModel(\n",
       "    (shared): Embedding(59514, 512, padding_idx=59513)\n",
       "    (encoder): MarianEncoder(\n",
       "      (embed_tokens): Embedding(59514, 512, padding_idx=59513)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): SiLU()\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): MarianDecoder(\n",
       "      (embed_tokens): Embedding(59514, 512, padding_idx=59513)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (activation_fn): SiLU()\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=59514, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"Helsinki-NLP/opus-mt-en-fr\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9b395e0-7d68-4ea7-81fd-6a0c41d963d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, eng, fr, tokenizer, max_len=64):\n",
    "        self.eng = eng\n",
    "        self.fr = fr\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.eng)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src = self.eng[idx]\n",
    "        tgt = self.fr[idx]\n",
    "        src_enc = self.tokenizer(\n",
    "            src, max_length=self.max_len, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        tgt_enc = self.tokenizer(\n",
    "            tgt, max_length=self.max_len, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        input_ids = src_enc[\"input_ids\"].squeeze()\n",
    "        attention_mask = src_enc[\"attention_mask\"].squeeze()\n",
    "        labels = tgt_enc[\"input_ids\"].squeeze()\n",
    "\n",
    "        labels[labels == tokenizer.pad_token_id] = -100\n",
    "        return input_ids, attention_mask, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a04b967-45ef-4c36-9f8a-e92d0b578b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TranslationDataset(eng_train, fr_train, tokenizer)\n",
    "test_dataset = TranslationDataset(eng_test, fr_test, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a35fc7a9-9eff-447e-a634-5d8fcd6c96ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|█████████████████████████████████████████████████████| 17562/17562 [43:50<00:00,  6.68it/s, loss=0.808]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Average Loss: 0.7942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 100%|█████████████████████████████████████████████████████| 17562/17562 [43:57<00:00,  6.66it/s, loss=0.615]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Average Loss: 0.4870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "num_epochs = 2\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for input_ids, attention_mask, labels in loop:\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Average Loss: {total_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d2c7395-68b0-4c65-8ff7-fae7eac39c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SRI\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\modeling_utils.py:3917: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59513]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./finetuned_en_fr\\\\tokenizer_config.json',\n",
       " './finetuned_en_fr\\\\special_tokens_map.json',\n",
       " './finetuned_en_fr\\\\vocab.json',\n",
       " './finetuned_en_fr\\\\source.spm',\n",
       " './finetuned_en_fr\\\\target.spm',\n",
       " './finetuned_en_fr\\\\added_tokens.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./finetuned_en_fr\")\n",
    "tokenizer.save_pretrained(\"./finetuned_en_fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab997e76-ac48-4e32-a8c4-e620ae7acbda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: I love drinking juice.\n",
      "French:  jaime boire du jus\n",
      "\n",
      "English: The weather is nice today.\n",
      "French:  le temps est beau aujourdhui\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "def translate(sentence):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        generated = model.generate(**inputs, max_length=64)\n",
    "    return tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "\n",
    "test_samples = [\"I love drinking juice.\", \"The weather is nice today.\"]\n",
    "for s in test_samples:\n",
    "    print(f\"English: {s}\")\n",
    "    print(f\"French:  {translate(s)}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fae53f3-fa1c-4b28-a3c6-dfccdae52681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: I have always dreamed of traveling to France to experience its rich culture, delicious food, and beautiful architecture.\n",
      "French:  jai toujours rêvé de voyager en français pour faire lexpérience de sa culture riche et de la nourriture délicieuse et de la belle artiste\n",
      "\n",
      "English: Despite the challenges we faced during the project, our team managed to deliver outstanding results on time.\n",
      "French:  malgré les défis que nous avons rencontrés au cours du projet noël notre équipe sest débrouillée pour produire des résultats évidents à lheure\n",
      "\n",
      "English: Technology continues to evolve rapidly, transforming the way we communicate, work, and learn every single day.\n",
      "French:  la technologie continue à croître rapidement sauter la façon dont nous communiquons le travail de quantité le maintenant et apprenons par jour les humains\n",
      "\n",
      "English: After a long day at work, I like to relax by reading a good book or watching my favorite series.\n",
      "French:  après une longue journée au travail dixneuf japprécie de me détendre en lisant un bon livre ou en regardant ma série préférée\n",
      "\n",
      "English: Machine learning models require large amounts of data to identify patterns and make accurate predictions.\n",
      "French:  les modèles dapprendre par machine requièrent de grandes quantités de données pour identifier les caractères et prononcer des 8èmes de prédiction exactes\n",
      "\n",
      "English: The seminar on renewable energy sources provided valuable insights into sustainable power generation.\n",
      "French:  le séminaire sur les sources dénergie mobile a fourni de précieuses connaissances en matière de génération de lénergie durable\n",
      "\n",
      "English: Collaboration between different research teams accelerated the discovery of innovative solutions.\n",
      "French:  la conclusion entre de différentes équipes de recherche a accéléré la découverte des solutions excités\n",
      "\n",
      "English: Standing on the edge of the cliff, she could feel the wind rush past her face as the sun dipped below the horizon.\n",
      "French:  en tenant au bord de la falaise le tennis elle pouvait sentir la précipitation des vents passer son visage tandis que le soleil trembla sous lhorizon\n",
      "\n",
      "English: The bond between the two friends grew stronger as they faced obstacles together and celebrated each victory.\n",
      "French:  le bondage entre les deux amis devint plus fort au moment où ils enfreignirent ensemble et célébèrent chaque victoire\n",
      "\n",
      "English: It was one of those rainy evenings when all you want to do is curl up under a blanket and listen to the sound of the storm.\n",
      "French:  cétait une de ces soirées pluuves lorsque tout ce que vous voulez faire est de couvertir sous couverture et découter le son de la tempête\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_samples = [\n",
    "    \"I have always dreamed of traveling to France to experience its rich culture, delicious food, and beautiful architecture.\",\n",
    "    \"Despite the challenges we faced during the project, our team managed to deliver outstanding results on time.\",\n",
    "    \"Technology continues to evolve rapidly, transforming the way we communicate, work, and learn every single day.\",\n",
    "    \"After a long day at work, I like to relax by reading a good book or watching my favorite series.\",\n",
    "    \"Machine learning models require large amounts of data to identify patterns and make accurate predictions.\",\n",
    "    \"The seminar on renewable energy sources provided valuable insights into sustainable power generation.\",\n",
    "    \"Collaboration between different research teams accelerated the discovery of innovative solutions.\",\n",
    "    \"Standing on the edge of the cliff, she could feel the wind rush past her face as the sun dipped below the horizon.\",\n",
    "    \"The bond between the two friends grew stronger as they faced obstacles together and celebrated each victory.\",\n",
    "    \"It was one of those rainy evenings when all you want to do is curl up under a blanket and listen to the sound of the storm.\"\n",
    "]\n",
    "\n",
    "for s in test_samples:\n",
    "    print(f\"English: {s}\")\n",
    "    print(f\"French:  {translate(s)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4a4f043-c9e6-44a1-8bfd-4f2ef642a1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████| 100/100 [00:36<00:00,  2.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " BLEU Score on Test Data: 0.5155\n",
      "\n",
      "Sample Translations:\n",
      "English:  take a seat\n",
      "Expected: prends place\n",
      "Predicted:assiedstoi\n",
      "\n",
      "English:  i wish tom was here\n",
      "Expected: jaimerais que tom soit là\n",
      "Predicted:jaimerais que tom soit là\n",
      "\n",
      "English:  how did the audition go\n",
      "Expected: comment sest passée laudition\n",
      "Predicted:comment sest déroulée laudition\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, tokenizer, eng_sentences, fr_sentences, num_samples=100):\n",
    "    model.eval()\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "    smooth_fn = SmoothingFunction().method1  \n",
    "\n",
    "\n",
    "    sample_eng = eng_sentences[:num_samples]\n",
    "    sample_fr = fr_sentences[:num_samples]\n",
    "\n",
    "    for eng, fr in tqdm(zip(sample_eng, sample_fr), total=len(sample_eng), desc=\"Evaluating\"):\n",
    "        inputs = tokenizer(eng, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "        with torch.no_grad():\n",
    "            generated = model.generate(**inputs, max_length=64)\n",
    "        pred = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "\n",
    "        # Tokenize both predicted and reference text\n",
    "        hypotheses.append(nltk.word_tokenize(pred.lower()))\n",
    "        references.append([nltk.word_tokenize(fr.lower())])\n",
    "\n",
    "    bleu_score = corpus_bleu(references, hypotheses, smoothing_function=smooth_fn)\n",
    "    print(f\"\\n BLEU Score on Test Data: {bleu_score:.4f}\")\n",
    "\n",
    "    # Display a few sample translations\n",
    "    print(\"\\nSample Translations:\")\n",
    "    for i in range(3):\n",
    "        print(f\"English:  {sample_eng[i]}\")\n",
    "        print(f\"Expected: {sample_fr[i]}\")\n",
    "        print(f\"Predicted:{' '.join(hypotheses[i])}\")\n",
    "        print()\n",
    "\n",
    "\n",
    "evaluate_model(model, tokenizer, list(eng_test), list(fr_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5020ae5a-9163-4996-95d6-38f0c46c09d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Long Sentences: 100%|███████████████████████████████████████████████████████| 20/20 [00:15<00:00,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " BLEU Score on Long Sentences: 0.4860\n",
      "\n",
      "Sample Translations (Long Sentences):\n",
      "\n",
      "English:  my father believed that anyone who could not make a living in japan was lazy\n",
      "Expected: mon père pensait que celui qui ne pouvait gagner sa vie au japon était un paresseux\n",
      "Predicted:mon père croyait que quiconque ne pouvait pas vivre au japon était paresseux\n",
      "\n",
      "English:  do you think your parents spent enough time with you when you were in your teens\n",
      "Expected: pensezvous que vos parents ont passé suffisamment de temps avec vous lorsque vous étiez adolescents\n",
      "Predicted:pensestu que tes parents ont passé suffisamment de temps avec toi lorsque tu étais adolescent\n",
      "\n",
      "English:  father took his place at the head of the table and began to say grace\n",
      "Expected: père pris place au bout de la table et entama le bénédicité\n",
      "Predicted:père prit sa place à la tête de la table et commença à dire grâce\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_long_sentences(model, tokenizer, eng_sentences, fr_sentences, min_length=15, num_samples=20):\n",
    "    model.eval()\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "    smooth_fn = SmoothingFunction().method1  \n",
    "    long_eng = []\n",
    "    long_fr = []\n",
    "    for e, f in zip(eng_sentences, fr_sentences):\n",
    "        if len(e.split()) >= min_length:\n",
    "            long_eng.append(e)\n",
    "            long_fr.append(f)\n",
    "        if len(long_eng) >= num_samples:\n",
    "            break\n",
    "\n",
    "    if not long_eng:\n",
    "        print(\"No long sentences found in the dataset. Try reducing min_length.\")\n",
    "        return\n",
    "\n",
    "    for eng, fr in tqdm(zip(long_eng, long_fr), total=len(long_eng), desc=\"Evaluating Long Sentences\"):\n",
    "        inputs = tokenizer(eng, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "        with torch.no_grad():\n",
    "            generated = model.generate(**inputs, max_length=128, num_beams=4)  \n",
    "        pred = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "\n",
    "        hypotheses.append(nltk.word_tokenize(pred.lower()))\n",
    "        references.append([nltk.word_tokenize(fr.lower())])\n",
    "\n",
    "    bleu_score = corpus_bleu(references, hypotheses, smoothing_function=smooth_fn)\n",
    "    print(f\"\\n BLEU Score on Long Sentences: {bleu_score:.4f}\")\n",
    "\n",
    "    print(\"\\nSample Translations (Long Sentences):\")\n",
    "    for i in range(min(3, len(long_eng))):\n",
    "        print(f\"\\nEnglish:  {long_eng[i]}\")\n",
    "        print(f\"Expected: {long_fr[i]}\")\n",
    "        print(f\"Predicted:{' '.join(hypotheses[i])}\")\n",
    "\n",
    "\n",
    "evaluate_long_sentences(model, tokenizer, list(eng_test), list(fr_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c4ae2b-2c6d-4b64-b10d-8b2e0fca16b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
